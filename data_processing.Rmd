---
title: "Data Processing"
author: "Ruoqi Zhang"
output:
  html_document:
    df_print: paged
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(cache=TRUE)
library(sf)
library(tidyverse)
library(gt)
library(sparklyr)
library(tictoc)
library(lubridate)
library(mapview)

# devtools::install_github("harryprince/geospark")

library(geospark)
library(stringr)
```

```{r sparklyr_setup, include=TRUE}

# Set environment variable for JDK 1.8 in order to point sparklyr package to the
# correct directory.

Sys.setenv(JAVA_HOME = "/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home")

# Configuring Sparklyr settings.

conf <- list()

# Assign 4 CPU cores, 10G memory to process the data.

conf$`sparklyr.cores.local` <- 4
conf$`sparklyr.shell.driver-memory` <- "10G"
conf$spark.memory.fraction <- 0.9

conf$spark.serializer <- "org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.registrator <- "org.datasyslab.geospark.serde.GeoSparkKryoRegistrator"

# Connect to local Spark instance.

sc <- spark_connect(master = "local",
                    config = conf)

register_gis(sc)

```

# Time Distribution in Week 1

```{r data_proc}

# Time the loading time for the csv files.

tic("sparkload")

# Read all WeiboScope csv files to WEIBOSCOPE_ALL variable through spark. The
# reason that we use Spark is that the files are too big to be stored in memory.
# Thus, we need a distributional computation platfrom to properly produce
# analysis on tis dataset. We process the dataset using spark, and generate
# dataframes that contains manageable metadata, and visualize it using ggplot.

WEIBOSCOPE_ALL <- spark_read_csv(sc,
  name = "weiboscope", 
  
  # Weiboscope data is stored under datasets/weiboscope_data in .csv formats.
  # Using wildcard, we read in all the data in that directory. Because
  # Weiboscope data already contains timestamp of the posts, we do not worry
  # about the filename.
  
  path = "datasets/weiboscope_data/*.csv",
  
  # Specifying the column types manually without letting Spark infering the
  # column types. The following column types are not specified under R coltypes,
  # but instead uses Spark SQL format.
  
  infer_schema = FALSE,
  columns = list(
    mid = "character",
    retweeted_status_mid = "character",
    uid = "character",
    retweeted_uid = "character",
    source = "character",
    image = "integer",
    text = "character",
    geo = "character",
    created_at = "timestamp",
    deleted_last_seen = "timestamp",
    permission_denied = "boolean"
  )
)
toc()

week1 <- read_csv("datasets/weiboscope_data/week1.csv",
         col_types = cols(
  mid = col_character(),
  retweeted_status_mid = col_character(),
  uid = col_character(),
  retweeted_uid = col_character(),
  source = col_character(),
  image = col_character(),
  text = col_character(),
  geo = col_character(),
  created_at = col_datetime(format = ""),
  deleted_last_seen = col_datetime(format = ""),
  permission_denied = col_logical()
))

```

```{r creation_time_dist}

tic("creation_time_dist")
creation_time_dist <- WEIBOSCOPE_ALL %>% 
  mutate(created_date = to_date(created_at)) %>% 
  group_by(created_date) %>% 
  summarize(post_created = n()) %>% 
  collect()
toc()

write_rds(creation_time_dist, "weibo_air_quality/data/creation_time_dist.rds")
```

```{r time_distribution_rds}

# 


week1 %>% 
  ggplot(aes(x = created_at)) +
    geom_freqpoly(binwidth = 600)

week1 %>% 
  mutate(created_at = ymd_hms(created_at))

week1 %>% 
  mutate(week_day = day(created_at))

week1 %>% 
  

tic("week1_spark_graph")
week1 %>% 
  ggplot(aes(x = created_at)) +
    geom_freqpoly(binwidth = 600) +
    theme_minimal() +
    labs(x = NULL,
         y = "Number of Posts Per Minute",
         title = paste("Variations In Number of Weibo Posts Across A Week"),
         subtitle = paste("Number of Weibo posts between", (week1 %>% arrange(created_at) %>% head(1) %>% select(created_at) %>% pull() %>% format(format = "%B %d, %Y")), "and", (week1 %>% arrange(desc(created_at)) %>% head(1) %>% select(created_at) %>% pull() %>% format(format = "%B %d, %Y")),"."),
         caption = "HKU WeiboScope Data: King-wa Fu, CH Chan, Michael Chau. Assessing Censorship \non Microblogs in China: Discriminatory Keyword Analysis and Impact Evaluation of the 'Real Name Registration' Policy.\nIEEE Internet Computing. 2013; 17(3): 42-50. http://doi.ieeecomputersociety.org/10.1109/MIC.2013.28")
toc()


tic("week1_graph")
week1_csv %>% 
  ggplot(aes(x = created_at)) +
    geom_freqpoly(binwidth = 600) +
    theme_minimal() +
    labs(x = NULL,
         y = "Number of Posts Per Minute",
         title = paste("Variations In Number of Weibo Posts Across A Week"),
         subtitle = paste("Number of Weibo posts between", (week1 %>% arrange(created_at) %>% head(1) %>% select(created_at) %>% pull() %>% format(format = "%B %d, %Y")), "and", (week1 %>% arrange(desc(created_at)) %>% head(1) %>% select(created_at) %>% pull() %>% format(format = "%B %d, %Y")),"."),
         caption = "HKU WeiboScope Data: King-wa Fu, CH Chan, Michael Chau. Assessing Censorship \non Microblogs in China: Discriminatory Keyword Analysis and Impact Evaluation of the 'Real Name Registration' Policy.\nIEEE Internet Computing. 2013; 17(3): 42-50. http://doi.ieeecomputersociety.org/10.1109/MIC.2013.28")
toc()

```

# Spatial Data In China

```{r china_provincial_map}

# We use GADM Global Topolotical dataset to plot out the provincial level map of
# China. However, due to the size of the map (~10M), plotting the original
# simple feature would take too much time. Therefore, we simplify the map while
# preserving its topology using st_simplify.

CHN_simplified <- read_rds("datasets/gadm36_rds/gadm36_CHN_1_sf.rds") %>% 
  st_simplify(preserveTopology = TRUE, dTolerance = 0.01)

# This reduced the size of the map, and makes it more mamagable to print.

object.size(CHN_simplified)

# We print out China's map for testing.

mapview(CHN_simplified)

```

```{r weibo_geo_dist}

WEIBOSCOPE_GEO <- WEIBOSCOPE_ALL %>% 
  
  # We are only concerned about the points that have geo attribute.
  
  filter(!is.na(geo)) %>%
  
  select(geo) %>% 
  
  collect()
  
write_rds(WEIBOSCOPE_GEO, "weibo_air_quality/data/weiboscope_geo_all.rds")

WEIBOSCOPE_GEO

  st_as_sfc(WEIBOSCOPE_GEO$geo, EWKB = TRUE, crs = 4326) %>% 
  
  st_

test <- week1 %>% 
  filter(!is.na(geo))


test_point <- st_as_sfc(test$geo, EWKB = TRUE, crs = 4326)

st_intersection(CHN_simplified, test_point)

st_transform(test_point$geo, 4326)

lengths(st_covers(CHN_simplified, test_point$geo))

mapview(list(CHN_simplified, test_point))

```

# Censorship Frequency

```{r}

permission_denied_dist <- WEIBOSCOPE_ALL %>% 
  filter(!is.na(permission_denied)) %>% 
  mutate(created_date = to_date(created_at)) %>% 
  group_by(created_date) %>% 
  summarize(deleted_count = n()) %>% 
  collect()

write_rds(permission_denied_dist, "weibo_air_quality/data/permission_denied_dist.rds")

permission_denied_all <- WEIBOSCOPE_ALL %>% 
  filter(!is.na(permission_denied)) %>% 
  collect()

write_rds(permission_denied_all, "weibo_air_quality/data/permission_denied_all.rds")


permission_denied_dist


```


# Keyword Frequency

```{r}

keywords <- c("环保", "环境保护", "空气质量", "雾霾", "PM2.5", "霾", "污染", "口罩")

frequency <- tibble(filter_keyword = character(),
                    created_date = date(),
                    post_created = integer())

for (keyword in keywords) {
  WEIBOSCOPE_ALL %>% 
    filter(str_detect(text, keyword)) %>% 
    mutate(created_date = to_date(created_at),
           filter_keyword = keyword) %>% 
    group_by(filter_keyword, created_date) %>% 
    summarize(post_created = n()) %>% 
    collect() %>% 
    bind_rows(frequency) -> frequency
}

write_rds(frequency, "weibo_air_quality/data/keywords_freq_dist.rds")


# for (keyword in keywords) {
#   week1 %>% 
#     filter(str_detect(text, keyword)) %>% 
#     mutate(created_date = as_date(created_at),
#            filter_keyword = keyword) %>% 
#     group_by(filter_keyword, created_date) %>% 
#     summarize(post_created = n()) %>% 
#     bind_rows(frequency) -> frequency
# }

frequency %>% 
  ggplot(aes(x = created_date, y = post_created, color = filter_keyword)) +
    geom_line()


```

