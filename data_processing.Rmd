---
title: "Data Processing"
author: "Ruoqi Zhang"
output:
  html_document:
    df_print: paged
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(cache=TRUE)
library(sf)
library(tidyverse)
library(gt)
library(sparklyr)
library(tictoc)
library(lubridate)
library(mapview)

# Installed through devtools::install_github("harryprince/geospark"). You would
# also need to install Geospark directly on the Spark instance you have.

library(geospark)
library(stringr)
library(gganimate)
library(janitor)

# For processing GEOTiff files and generating PM2.5 Map.

library(raster)
library(rasterVis)
library(rgdal)
library(maps)
library(mapdata)
library(maptools)
```

```{r sparklyr_setup, include=TRUE}

# Set environment variable for JDK 1.8 in order to point sparklyr package to the
# correct directory.

Sys.setenv(JAVA_HOME = "/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home")

# Configuring Sparklyr settings.

conf <- list()

# Usually I would set the local cores to 8 and unset executor cores so that
# Spark utilizes the best options possible on the setup. However, there **might** be
# an issue with the thread-safety of Geospark. Thus, I leave the options to only
# assign one core to an executor here for debugging purposes.

conf$`sparklyr.cores.local` <- 8
# conf$spark.executor.cores <- 1
conf$`sparklyr.shell.driver-memory` <- "10G"
conf$spark.memory.fraction <- 0.9

# Switching the default serializer to Kryo Serializer and register it to
# Geospark. Kyro Serializer allows for more efficient serializing.

conf$spark.serializer <- "org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.registrator <- "org.datasyslab.geospark.serde.GeoSparkKryoRegistrator"

# Connect to local Spark instance.

sc <- spark_connect(master = "local",
                    config = conf)

# GeoSpark registration.

register_gis(sc)

```

# Time Distribution in Week 1

```{r data_proc}

# Time the loading time for the csv files.

tic("weiboscope_load_full")

# Read all WeiboScope csv files to WEIBOSCOPE_ALL variable through spark. The
# reason that we use Spark is that the files are too big to be stored in memory.
# Thus, we need a distributional computation platfrom to properly produce
# analysis on tis dataset. We process the dataset using spark, and generate
# dataframes that contains manageable metadata, and visualize it using ggplot.

WEIBOSCOPE_ALL <- spark_read_csv(sc,
  name = "weiboscope", 
  
  # Weiboscope data is stored under datasets/weiboscope_data in .csv formats.
  # Using wildcard, we read in all the data in that directory. Because
  # Weiboscope data already contains timestamp of the posts, we do not worry
  # about the filename.
  
  path = "datasets/weiboscope_data/*.csv",
  
  # Specifying the column types manually without letting Spark infering the
  # column types. The following column types are not specified under R coltypes,
  # but instead uses Spark SQL format.
  
  infer_schema = FALSE,
  columns = list(
    mid = "character",
    retweeted_status_mid = "character",
    uid = "character",
    retweeted_uid = "character",
    source = "character",
    image = "integer",
    text = "character",
    geo = "character",
    created_at = "timestamp",
    deleted_last_seen = "timestamp",
    permission_denied = "boolean"
  )
)
toc()

# It would usually takes ~20 minutes for Sparklyr to read in all the csv files.
# We need something more efficient to store this dataset. Thus, after running
# this for the first time, we store the intermediate dataset into a parquet file
# - a file format designed for Spark and is much more efficient in terms of
# storage usage and reading speed.

spark_write_parquet(WEIBOSCOPE_ALL, "spark_parquets/weiboscope_all_before_partition.parquet")

# Usually would run this, read the parquet files directly.

WEIBOSCOPE_ALL <- spark_read_parquet(sc, name = "weiboscope", path = "spark_parquets/weiboscope_all.parquet/")

# This chunk of code is for small-scale testing of WeiboScope Open data in
# Spark. Preserved for later use.

# tic("weiboscope_load_test")
# WEIBOSCOPE_TEST <- spark_read_csv(sc,
#   name = "weiboscope_test", 
#   
#   # Weiboscope data is stored under datasets/weiboscope_data in .csv formats.
#   # Using wildcard, we read in all the data in that directory. Because
#   # Weiboscope data already contains timestamp of the posts, we do not worry
#   # about the filename.
#   
#   path = "datasets/weiboscope_data/week4.csv",
#   
#   # Specifying the column types manually without letting Spark infering the
#   # column types. The following column types are not specified under R coltypes,
#   # but instead uses Spark SQL format.
#   
#   infer_schema = FALSE,
#   columns = list(
#     mid = "character",
#     retweeted_status_mid = "character",
#     uid = "character",
#     retweeted_uid = "character",
#     source = "character",
#     image = "integer",
#     text = "character",
#     geo = "character",
#     created_at = "timestamp",
#     deleted_last_seen = "timestamp",
#     permission_denied = "boolean"
#   )
# )
# toc()

```

```{r creation_time_dist}

# Using all WeiboScope data in Spark environment, we hope to calculate how many
# posts are posted everyday in 2012.

tic("creation_time_dist")
creation_time_dist <- WEIBOSCOPE_ALL %>% 
  
  #Due to Spark SQL settings, we imported created_at variable as a timestamp.
  #Because of Spark SQL grammar constraint, we cannot use as_date() in
  #Lubridate. We instead use to_date function provided in Spark's HIVE
  #functions. By using this, we obtain the date portion of created_at varibale.
  
  mutate(created_date = to_date(created_at)) %>% 
  group_by(created_date) %>% 
  
  # Calculate how many posts are created in each day.
  
  summarize(post_created = n()) %>% 
  
  # Function specific to Spark. Use collect() to calculate the result and put
  # the result into R workspace.
  
  collect()
toc()

# This takes a lot of computational power and time to calculate. Thus, we save
# our result into an RDS file for our Shiny app to use.

write_rds(creation_time_dist, "weibo_air_quality/data/creation_time_dist.rds")
```

# Spatial Data In China

```{r china_provincial_map}

# We use GADM Global Topolotical dataset to plot out the provincial level map of
# China. However, due to the size of the map (~10M), plotting the original
# simple feature would take too much time. Therefore, we simplify the map while
# preserving its topology using st_simplify.

CHN_simplified <- read_rds("datasets/gadm36_rds/gadm36_CHN_1_sf.rds") %>% 
  st_simplify(preserveTopology = TRUE, dTolerance = 0.01)

# We load the 
  
CHN_2 <- read_rds("datasets/gadm36_rds/gadm36_CHN_2_sf.rds")

# This reduced the size of the map, and makes it more mamagable to print. To
# check whether it's feasible to print, we manually examine the size of the
# simplified map.

object.size(CHN_simplified)

# We print out China's map for testing.

mapview(CHN_simplified)

mapview(year_total_geo, zcol = "total_posts",
        layer.name = "Number of Posts",
        viewer.suppress = TRUE)

```



```{r weibo_geo_dist}

# WeiboScope Open's Geographical data is coded as character in the format of
# "POINT(0 0)". We would like to import this into our Spark environment for
# calculation. However, we cannot use sf to directly calculate due to the memory
# constraint. To achieve this, we use Geospark package to complete the
# calculation directly in Spark.

WEIBOSCOPE_GEO_WKT <- WEIBOSCOPE_ALL %>% 
  
  # In the Weiboscope Data, because the longitude and latitude is coded as
  # string character, we need to extract them into separate variables in order
  # to manipulate the data. Spark does not support stringr functions, therefore,
  # we are using regex expressions (Java format, since Spark is based on Java)
  # to extract them.
  
  mutate(longitude = regexp_extract(geo, '[(](.*?)\\\\s'),
         latitude = regexp_extract(geo, '\\\\s(.*?)[)]' )) %>% 
  
  # We filter out the geo data that is NA. And remove rows that contains
  # erroneous data. We also remove obviously unrealisitc points.
  
  filter(!is.na(geo),
         
         # There are some instances where the geo column contains information
         # that is not geo information at all. We filter for the rows that
         # confrom to the format.
         
         str_detect(geo, "POINT"),
         
         # This is obviously impossible
         
         geo != "POINT(0 0)",
         
         # There are some data points in the WeiboScope data that is outside of
         # possible bounds, causing runtime errors in Spark. We filter out those
         # data.
         
         between(longitude, 0, 180),
         between(latitude, -90, 90)) %>% 
  
  # Within Spark, we use GeoSpark function st_geomfromwkt to transform character
  # string contained in geo to a Simple Feature object within Spark supported by
  # Geospark. We store the result of this transformation into variable
  # weibo_geo_wkt.
  
  mutate(weibo_geo_wkt = st_geomfromwkt(geo)) %>% 
  filter(!is.na(weibo_geo_wkt))

# Registering the table in the Spark workplace and give it a name.

sdf_register(WEIBOSCOPE_GEO_WKT, name = "WEIBOSCOPE_GEO_WKT")

# Write the intermediate result to parquet format.

spark_write_parquet(WEIBOSCOPE_GEO_WKT, "spark_parquets/weiboscope_geo_before_partition.parquet")

# Now, because the previous steps involve filtering a significant chunk of data.
# The original partition assigned by Spark is no longer optimal for future
# operations. We would like to optimize this operations. Therefore, we first get
# the number of partitions of the original Spark table. Then, comparing the rows
# of the WEIBOSCOPE_ALL dataframe and geo filtered dataframe, we proportionally
# reduce the number of partitions, which gives us a resulting partition of 35.

sdf_num_partitions(WEIBOSCOPE_ALL)
sdf_num_partitions(WEIBOSCOPE_GEO_WKT)
sdf_nrow(WEIBOSCOPE_ALL)
sdf_nrow(WEIBOSCOPE_GEO_WKT)
WEIBOSCOPE_GEO_WKT <- sdf_repartition(WEIBOSCOPE_GEO_WKT, partitions = 35)

# Write the intermediate result to parquet format.

spark_write_parquet(WEIBOSCOPE_GEO_WKT, "spark_parquets/weiboscope_geo_after_partition.parquet")

# Then, we need to transform the simplified Chinese map into a format that is
# compatible to GeoSpark. To achieve this, we first convert it into a dataframe,
# and transform its geometry into character strings. We then remove its geometry
# column that contains value incompatible with Spark.

WEIBOSCOPE_GEO_WKT <- spark_read_parquet(sc, name = "weiboscope_geo_wkt", path = "spark_parquets/weiboscope_geo_after_partition.parquet/")

CHN_GeoSpark_Map_df <- as_data_frame(CHN_2) %>% 
  mutate(chn_map_geom = st_as_text(geometry),
         geometry = NULL)

# CHN_GeoSpark_Map_df <- as_data_frame(CHN_simplified) %>% 
#   mutate(chn_map_geom = st_as_text(geometry),
#          geometry = NULL)

# Put the Chinese Map into Spark storage.

CHN_GeoSpark_Map_spark <- copy_to(sc, CHN_GeoSpark_Map_df)

# Within Spark's storage, we transform the the character that contains the
# geographical boundary of China's administrative regions into Simple Feature
# objects that can be processed in GeoSpark.

CHN_GeoSpark_Map <- mutate(CHN_GeoSpark_Map_spark, chn_geo_wkt = st_geomfromwkt(chn_map_geom))

# sdf_register(CHN_GeoSpark_Map, name = "CHN_GeoSpark_Map")

CHN_GeoSpark_Map


# We then use Geospark's st_join function to calculate their intersections. We
# want to find out the number of posts in each province in each month.
# Therefore, we specify join condition as contain, and let Spark calculate which
# administrative zone a specific geotagged post is in.

# We store this as an intermediate result, not collecting this into R workspace
# for further data wrangling and analysis.

geo_distribution <- st_join(CHN_GeoSpark_Map,
                  WEIBOSCOPE_GEO_WKT,
                  
                  # We use an sql operation provided by GeoSpark here. By using
                  # st_contains, we are joining the rows in which the Weibo
                  # posts location is located within a Chinese province.
                  
                  join = sql("st_contains(chn_geo_wkt, weibo_geo_wkt)"))

# Registering the Spark table with a name in the Spark workspace.

sdf_register(geo_distribution, name = "geo_distribution")

# Write the intermediate results to parquet files.

spark_write_parquet(geo_distribution, path = "spark_parquets/geo_distribution.parquet")

# In this step, we hope to calculate the volume of Weibo posts in different
# provinces across 2012.

monthly_distribution <- st_join(CHN_GeoSpark_Map,
                  WEIBOSCOPE_GEO_WKT,
                  join = sql("st_contains(chn_geo_wkt, weibo_geo_wkt)")) %>% 
  mutate(created_month = month(to_date(created_at))) %>% 
  group_by(NAME_1, created_month) %>%
  summarise(cnt = n()) %>% 
  collect()

monthly_distribution <- monthly_distribution %>% 
  group_by(NAME_1) %>% 
  summarize(total_posts = sum(cnt)) %>% 
  full_join(CHN_simplified) %>% 
  st_as_sf()

write_rds(year_total_geo, "weibo_air_quality/data/year_total_geo.rds")

year_total_geo <- read_rds("weibo_air_quality/data/year_total_geo.rds")

str_split("安徽|安徽", pattern = fixed("|"))

as.character(map_chr(str_split("安徽|安徽", pattern = fixed("|")), c(2)))

year_total_geo %>% 
  mutate(Name = NAME_1,
         `Name in Chinese` = NL_NAME_1)


geo_time_count <- geo_distribution %>% 
  mutate(created_month = month(to_date(created_at))) %>% 
  group_by(NAME_1, NL_NAME_1, ENGTYPE_1, created_month) %>%
  summarise(cnt = n()) %>% 
  collect()

censored_geo_post <- geo_distribution %>% 
  filter(!is.na(permission_denied)) %>% 
  collect()

```

# Censorship Frequency

```{r}

permission_denied_dist <- WEIBOSCOPE_ALL %>% 
  filter(!is.na(permission_denied)) %>% 
  mutate(created_date = to_date(created_at)) %>% 
  group_by(created_date) %>% 
  summarize(deleted_count = n()) %>% 
  collect()

write_rds(permission_denied_dist, "weibo_air_quality/data/permission_denied_dist.rds")

permission_denied_dist <- read_rds("weibo_air_quality/data/permission_denied_dist.rds")

censorship_dist_plot <- permission_denied_dist %>% 
  ggplot(aes(x = created_date, y = deleted_count)) +
    geom_line(color = "#d11141") +
    transition_reveal(created_date) +
    theme_minimal() +
    labs(title = "",
         x = NULL,
         y = "Number of Monitored Censored Posts",
         caption = "Source: Open WeiboScope/Hong Kong University") +
    theme(panel.grid.minor = element_blank())

animate(censorship_dist_plot, 
        nframes = 365,
        fps = 30,
        duration = 8,
        end_pause = 150,
        width = 1000,
        height = 500)

anim_save("weibo_air_quality/censorship_dist_plot.gif", animation = censorship_dist_plot,
          nframes = 365,
          fps = 30,
          duration = 10,
          end_pause = 150,
          width = 800,
          height = 400)


permission_denied_all <- WEIBOSCOPE_ALL %>% 
  filter(!is.na(permission_denied)) %>% 
  collect()

write_rds(permission_denied_all, "weibo_air_quality/data/permission_denied_all.rds")


permission_denied_dist


```


# Keyword Frequency

```{r}

keywords <- c("环保", "环境保护", "空气质量", "雾霾", "PM2.5", "霾", "污染", "口罩")

frequency <- tibble(filter_keyword = character(),
                    created_date = date(),
                    post_created = integer())

for (keyword in keywords) {
  WEIBOSCOPE_ALL %>% 
    filter(str_detect(text, keyword)) %>% 
    mutate(created_date = to_date(created_at),
           filter_keyword = keyword) %>% 
    group_by(filter_keyword, created_date) %>% 
    summarize(post_created = n()) %>% 
    collect() %>% 
    bind_rows(frequency) -> frequency
}

write_rds(frequency, "weibo_air_quality/data/keywords_freq_dist.rds")

frequency <- read_rds("weibo_air_quality/data/keywords_freq_dist.rds")

# for (keyword in keywords) {
#   week1 %>% 
#     filter(str_detect(text, keyword)) %>% 
#     mutate(created_date = as_date(created_at),
#            filter_keyword = keyword) %>% 
#     group_by(filter_keyword, created_date) %>% 
#     summarize(post_created = n()) %>% 
#     bind_rows(frequency) -> frequency
# }

frequency %>% 
  mutate(filter_keyword = factor(filter_keyword,
                                 levels = c("环保", "环境保护", "空气质量", "雾霾", "PM2.5", "霾", "污染", "口罩"),
                                 labels = c("环保 (Environmental Protection)", "环境保护 (Environmental Protection)",
                                            "空气质量 (Air Quality)", "雾霾 (Smog)", "PM2.5", "霾 (Smog)", 
                                            "污染 (Pollution)", "口罩 (Facemask)"))) %>%
  ggplot(aes(x = created_date, y = post_created, color = filter_keyword)) +
    geom_line() +
    theme_minimal() +
    theme(legend.text=element_text(family="STKaiti")) +
    


```

```{r}

pm25_1998 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-1998-geotiff/gwr_pm25_1998.tif")
pm25_1999 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-1999-geotiff/gwr_pm25_1999.tif")
pm25_2000 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2000-geotiff/gwr_pm25_2000.tif")
pm25_2001 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2001-geotiff/gwr_pm25_2001.tif")
pm25_2002 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2002-geotiff/gwr_pm25_2002.tif")
pm25_2003 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2003-geotiff/gwr_pm25_2003.tif")
pm25_2004 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2004-geotiff/gwr_pm25_2004.tif")
pm25_2005 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2005-geotiff/gwr_pm25_2005.tif")
pm25_2006 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2006-geotiff/gwr_pm25_2006.tif")
pm25_2007 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2007-geotiff/gwr_pm25_2007.tif")
pm25_2008 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2008-geotiff/gwr_pm25_2008.tif")
pm25_2009 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2009-geotiff/gwr_pm25_2009.tif")
pm25_2010 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2010-geotiff/gwr_pm25_2010.tif")
pm25_2011 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2011-geotiff/gwr_pm25_2011.tif")
pm25_2012 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2012-geotiff/gwr_pm25_2012.tif")
pm25_2013 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2013-geotiff/gwr_pm25_2013.tif")
pm25_2014 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2014-geotiff/gwr_pm25_2014.tif")
pm25_2015 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2015-geotiff/gwr_pm25_2015.tif")
pm25_2016 <- raster("datasets/sdei-pm25/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-2016-geotiff/gwr_pm25_2016.tif")

pm25_list <- c("pm25_1998", "pm25_1999", "pm25_2000", "pm25_2001", "pm25_2002", "pm25_2003", "pm25_2004", "pm25_2005", "pm25_2006", "pm25_2007", "pm25_2008", "pm25_2009", "pm25_2010", "pm25_2011", "pm25_2012", "pm25_2013", "pm25_2014", "pm25_2015", "pm25_2016")

colr <- colorRampPalette(rev(brewer.pal(11, 'RdYlBu')))

padding <- list(layout.heights = list(top.padding = 0), 
                layout.widths = list( 
                        left.padding = 0, 
                        right.padding = 0) 
                ) 

for (year in pm25_list) {
  
  png(filename = paste0("weibo_air_quality/pm25_graphs/", year, ".png"), width = 3000, height = 1000,
      res = 300)
  
  plot <- levelplot(
    eval(as.name(year)),
    main = paste("Air Pollution (PM2.5) Concentration in China,", str_split(year, pattern = "_")[[1]][2]),
    xlim = c(70, 140),
    ylim = c(10, 60),
    xlab = NULL,
    ylab = NULL,
    alpha.regions = 0.8,
    maxpixels = 2e6,
    margin = FALSE,
    colorkey = list(
      # space='bottom',                   # plot legend at bottom
      title = "mg / cubic m"
    ),
    # par.settings=list(
    #         axis.line=list(col='transparent') # suppress axes and legend outline
    # ),
    scales=list(draw=FALSE),            # suppress axis labels
    col.regions=colr,                   # colour ramp
    at=seq(0, 110, by = 10),
    par.settings = padding
  ) +
    layer(
      sp.polygons(as_Spatial(CHN_simplified))
    )
  
  print(plot)
  
  dev.off()

}

```

# Economic Graphs

```{r economic_data_proc}

indicators <- read_csv("datasets/World_Development_Indicators/03d8a56f-acd9-4053-adfd-b596413871f5_Data.csv") %>% 
  clean_names()

unique(indicators$series_name)

selected_indicators <- c("GDP (current US$)", "Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)", "Urban population (% of total)", "Individuals using the Internet (% of population)", "School enrollment, primary (% gross)", "School enrollment, secondary (% gross)")

cleaned_indicators <- indicators %>% 
  filter(series_name %in% selected_indicators) %>% 
  gather(key = "year", value = "value", x1960_yr1960:x2017_yr2017) %>% 
  dplyr::select(-country_name, -country_code, -x2018_yr2018, -series_code) %>% 
  mutate(year = map_chr(str_split(year, pattern = "_"), c(1)),
         year = as.integer(str_remove(year, "x")))

write_rds(cleaned_indicators, "weibo_air_quality/data/development_indicators.rds")

china_gdp <- read_csv("datasets/World_Development_Indicators/gdp_dollar.csv",
                               skip = 3) %>% 
  clean_names() %>% 
  filter(country_name == "China") %>% 
  select(-x2018, -x64) %>% 
  gather(key = "year", "value" = gdp, x1960:x2017) %>% 
  mutate(year = as.integer(str_remove(year, "x")))

write_rds(china_gdp, "weibo_air_quality/data/china_gdp.rds")

china_gdp %>% 
  ggplot(aes(x = year, y = gdp)) +
    geom_line() +
    geom_point(size = 0.5) +
    theme_minimal() +
    labs(title = "China's Post-Reform Economy: Rapid Development",
         subtitle = "China's GDP (in current USD) from 1960 to 2017",
         x = NULL) +
    scale_y_continuous(name = "GDP (in trillion dollar)",
                       breaks = seq(0, 14e12, by = 2e12),
                       labels = paste0("$",seq(0, 14, by = 2),"tn")) +
    scale_x_continuous(breaks = seq(1960, 2017, by = 5))

cleaned_indicators %>% 
  filter(series_name == "Urban population (% of total)") %>% 
  mutate(value = as.numeric(value)) %>% 
  ggplot(aes(x = year, y = value)) +
    geom_line() +
    geom_point(size = 0.5) +
    theme_minimal() +
    labs(title = "Into an Urban Society",
         subtitle = "China became predominantly urban in 2011",
         x = NULL) +
    geom_hline(yintercept = 50, linetype="dashed", color = "red") +
    scale_y_continuous(name = "Urban population (% of total)",
                       limits = c(0, 100),
                       breaks = seq(0, 100, by = 10),
                       labels = paste0(seq(0, 100, by = 10),"%")) +
    scale_x_continuous(breaks = seq(1960, 2017, by = 5))

```

```{r}
library(googleLanguageR)
gl_auth("weibo_air_quality/Transcription-675d1c2f9aef.json")
censored <- read_rds("weibo_air_quality/data/permission_denied_all.rds")
sample_n(censored, 1) %>% 
  dplyr::select(text) %>% 
  pull() %>% 
  gl_translate(target = "en")

```

